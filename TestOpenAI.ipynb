{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c62cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending query: 陈嗣元是少将么\n",
      "Response: ChatCompletion(id='chatcmpl-CH6YT0Ls8m0d0Iulyl5N7bpavJDgt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='是的，陈嗣元是中国人民解放军的少将。', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1758192809, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=27, total_tokens=48, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Content: 是的，陈嗣元是中国人民解放军的少将。\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "# Create OpenAI API interface\n",
    "import os\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from openai import OpenAIError\n",
    "\n",
    "# Add project root directory to Python path\n",
    "sys.path.append('/data/CSY/autodefense/AutoDefense')\n",
    "\n",
    "# Set proxy environment variables for network access\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "\n",
    "class OpenAI_API(object):\n",
    "    def __init__(self, api_key, model_name=\"gpt-3.5-turbo\"):\n",
    "        # Initialize with the standard OpenAI client\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def get_response(self, prompt, max_tokens=500, temperature=0.7, \n",
    "                     top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "        try:\n",
    "            # Use the standard chat completion API\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty\n",
    "            )\n",
    "            return response\n",
    "        except OpenAIError as e:\n",
    "            print(f\"OpenAI API Error: {e}\")\n",
    "            # Try fallback model if available\n",
    "            if self.model_name != \"gpt-3.5-turbo\":\n",
    "                print(\"Trying fallback model: gpt-3.5-turbo\")\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=max_tokens,\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "                    return response\n",
    "                except OpenAIError as fallback_error:\n",
    "                    print(f\"Fallback model also failed: {fallback_error}\")\n",
    "                    raise\n",
    "            raise\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Get API key from environment or use default\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "        \n",
    "        # Initialize OpenAI API with the correct model name\n",
    "        # Note: The standard model name is \"gpt-3.5-turbo\", not \"gpt-35-turbo\"\n",
    "        openai_api = OpenAI_API(api_key, model_name=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Test query\n",
    "        prompt = \"陈嗣元是少将么\"\n",
    "        print(f\"Sending query: {prompt}\")\n",
    "        result = openai_api.get_response(prompt, max_tokens=150)\n",
    "        \n",
    "        # Display response\n",
    "        print(f\"Response: {result}\")\n",
    "        if hasattr(result, 'choices') and result.choices:\n",
    "            print(f\"Content: {result.choices[0].message.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Main program error: {e}\")\n",
    "        print(\"Possible issues to check:\")\n",
    "        print(\"1. API key is valid and has access to the model\")\n",
    "        print(\"2. Network connection and proxy settings are correct\")\n",
    "        print(\"3. You're using the correct model name (gpt-3.5-turbo is standard)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodefense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
